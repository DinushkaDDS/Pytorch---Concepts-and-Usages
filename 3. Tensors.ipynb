{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors in Pytorch\n",
    "\n",
    "This is the main multidimensional data structure used in pytorch to do its calculations. Even though pytorch supports numpy arrays they are not suitable for deep learning related works due to multiple problems. Tensors fix those problems by providing additional features such as distributed operations on multiple devices/machines, GPU operations support, computation graph support etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((3, 2))  # Similar to numpy ones create a vector with 1 value to the given dimension.\n",
    "a[2]  # We can access elements just as we access them in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like numpy, torch tensors are saved in memory as contiguous memory chunks. Which basically means CPU caching techniques, parrallel processing techniques can be applied easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([1,2,3,4,5,6,7,8,9])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_2d = torch.tensor([[1,2],[3,4],[5,6]])\n",
    "points_2d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_2d.shape # Just like numpy XD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that, all the indexing operations return a tensor object. But it does not mean torch creates a new tensor object for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_2d[1,1] # Indexing the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below includes example slicing operations for tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_2d[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_2d[1: , 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2],\n",
       "         [3, 4],\n",
       "         [5, 6]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_2d[None] # This is special, \n",
    "                    # it adds additional dimension around the data. \n",
    "                    # Similar to the `unsqueeze` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2],\n",
       "         [3, 4],\n",
       "         [5, 6]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(points_2d, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In torch we can give names to the the tensor dimensions(Experimental feature!). This is helpful in keeping track of dimensions throughout complex operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = torch.rand((3, 25, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ddsdi\\.virtualenvs\\Pytorch_-_Use_Case_Exploration-FSDBtUdB\\lib\\site-packages\\torch\\_tensor.py:905: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/core/TensorImpl.h:1408.)\n",
      "  return super(Tensor, self).refine_names(names)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5485, 0.5179, 0.3718,  ..., 0.3386, 0.7032, 0.0538],\n",
       "         [0.8161, 0.4748, 0.1484,  ..., 0.2440, 0.2855, 0.6904],\n",
       "         [0.8604, 0.7152, 0.5117,  ..., 0.4493, 0.3384, 0.6642],\n",
       "         ...,\n",
       "         [0.0402, 0.8522, 0.2582,  ..., 0.3640, 0.1131, 0.1111],\n",
       "         [0.4787, 0.3820, 0.9931,  ..., 0.5017, 0.7539, 0.1426],\n",
       "         [0.4462, 0.0910, 0.1155,  ..., 0.6020, 0.9075, 0.0931]],\n",
       "\n",
       "        [[0.5188, 0.7891, 0.9740,  ..., 0.3745, 0.2788, 0.7585],\n",
       "         [0.1483, 0.9685, 0.5006,  ..., 0.8362, 0.0375, 0.1448],\n",
       "         [0.7215, 0.0814, 0.7408,  ..., 0.5229, 0.2238, 0.3893],\n",
       "         ...,\n",
       "         [0.8274, 0.1469, 0.4439,  ..., 0.6575, 0.4228, 0.7071],\n",
       "         [0.3851, 0.8331, 0.9895,  ..., 0.6077, 0.5968, 0.5964],\n",
       "         [0.3364, 0.1816, 0.0257,  ..., 0.9907, 0.1269, 0.1198]],\n",
       "\n",
       "        [[0.4230, 0.3410, 0.4444,  ..., 0.2268, 0.0655, 0.3102],\n",
       "         [0.7054, 0.6552, 0.7055,  ..., 0.6521, 0.8026, 0.0387],\n",
       "         [0.9557, 0.2813, 0.8386,  ..., 0.5676, 0.3350, 0.6215],\n",
       "         ...,\n",
       "         [0.3910, 0.4216, 0.0971,  ..., 0.3674, 0.6338, 0.9114],\n",
       "         [0.1725, 0.9180, 0.8769,  ..., 0.8817, 0.9200, 0.8566],\n",
       "         [0.0255, 0.6748, 0.8937,  ..., 0.8739, 0.7717, 0.0778]]],\n",
       "       names=('Channel', 'Height', 'Width'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img.refine_names(..., 'Channel', 'Height', 'Width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python standard data types are not optimal for large numerical operations. \n",
    "\n",
    "1. They are objects.\n",
    "2. Python lists are collections of objects which does not have contigious memory allocation.\n",
    "3. Python interpreter is not optimal compared to more specialized compiled code.\n",
    "\n",
    "Therefore just as numpy, cython torch tensors use ctype data or more efficient low level numerical data structures.\n",
    "\n",
    "In tensor constructor we can pass the datatype using `dtype` parameter. Some of the possible values are as below.\n",
    "\n",
    "* torch.float (float32)\n",
    "* torch.double (float64)\n",
    "* torch.int8\n",
    "* torch.uint8\n",
    "* torch.int\n",
    "* torch.long (int64)\n",
    "* torch.bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((5,5), dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((5,5)).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((5,5)).to(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about the pytorch tensor usage refer the [documentation](http://pytorch.org/docs).\n",
    "\n",
    "But as a general overview how tensors work under the hood is as follows.\n",
    "\n",
    "- Theres a torch class named `torch.Storage`. This is a one dimensional array of numerical data type spread over a contigious memory space.\n",
    "- Tensors are a type of view which utilizes this one dimensional data structure in a clever manner to provide fast and efficient indexes. (The indexing mechanism is kinda similar to how we can store a binary tree in a list.)\n",
    "\n",
    "Because of that, underline memory is allocated once. But we can index, reshape very fast since we are only using views on top of actual data. Very Cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Pytorch_-_Use_Case_Exploration-FSDBtUdB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1153c933a691d5b7872e50b0da6113c7d23c17547b77ca34e6f51a7318bb0ae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
