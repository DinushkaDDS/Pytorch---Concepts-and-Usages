{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Autograd\n",
    "\n",
    "Deep Learning means basically estimating some parameters set that would map a given input data distribution to a output distribution in an acceptable manner. No matter what the usecase is, in general sense this is what we try to achieve.\n",
    "\n",
    "But as we know this require linear algebra and differenciation (means maths), and pytorch inherently support these mathematical operations.\n",
    "\n",
    "To understand how these are working, lets check on a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here assume we have a set of values `t_c` which should some how relate to another set of values `t_u`. Now our task is to identify the relationship between 2 such that if we have a `t_c` value we can predict the corresponding `t_u` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(t_u, t_c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can this linear model would fit here. So now what we do is basically try to fix a random linear model to this data and see how much error it would yield compared to actual data. From that error value we can identify how much we want to rectify our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_vals, w, b):\n",
    "    return w*input_vals + b\n",
    "\n",
    "def calc_error(pred_vals, true_vals):\n",
    "    return ((pred_vals-true_vals)**2).mean() # Since we are using tensors, or we can use numpy functions here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a model and a way to calculate errors. But to improve our model so that we can get lower error rate we need to do something else. Basically we have to get how each parameters effect the error outcome and adjust the parameters to reduce that effect. (Error minimization using derivative - gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_error_pred(pred_vals, true_vals):\n",
    "    diff = 2*(pred_vals-true_vals)/pred_vals.size(0)\n",
    "    return diff\n",
    "\n",
    "# linear function derivative over b\n",
    "def d_pred_b(pred_vals, b):\n",
    "    return 1.0\n",
    "\n",
    "# linear function derivative over w\n",
    "def d_pred_w(pred_vals, w):\n",
    "    return pred_vals\n",
    "\n",
    "def grad_calculation(pred_vals, true_vals, params):\n",
    "    w, b = params\n",
    "    d_l_p = d_error_pred(pred_vals, true_vals)\n",
    "\n",
    "    d_p_b = d_pred_b(pred_vals, b)\n",
    "    d_p_w = d_pred_w(pred_vals, w)\n",
    "\n",
    "    # partial derivatives of Loss\n",
    "    d_l_w = d_l_p*d_p_w\n",
    "    d_l_b = d_l_p*d_p_b\n",
    "\n",
    "    return torch.stack([d_l_w.sum(), d_l_b.sum()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wonder why we are separately calculating the differenciation functions for this is simple linear equation, answer is for demonstration. The above separation of differentiation is due to partial derivatives for error function.\n",
    "\n",
    "<center><image src=\"./contents/1.jpg\" width=\"500px\"/></center>\n",
    "\n",
    "\n",
    "As you can probably remember in DL there are annoyingly large number of notations and these notations cause various confusions (like outputs after activations A, layers l, outputs before activations z etc). So to be aligned with those notations above calculations were done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_iter, learning_rate, params, input_vals, true_vals):\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        w, b = params\n",
    "        preds = model(input_vals, w, b)\n",
    "        error = calc_error(preds, true_vals)\n",
    "\n",
    "        # Backpropagation or differenciation process\n",
    "        gradients = grad_calculation(preds, true_vals, params)\n",
    "\n",
    "        # Updating parameters to compensate for error gradients\n",
    "        params = params - learning_rate*gradients\n",
    "\n",
    "        if(i%500==0):\n",
    "            print(f\"Iteration {i+1} completed with error value of {error} and gradient {gradients}\")\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 completed with error value of 1763.884765625 and gradient tensor([4517.2964,   82.6000])\n",
      "Iteration 11 completed with error value of 33.027652740478516 and gradient tensor([57.7695,  6.8204])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2484, -0.0257])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(15, 1e-4, torch.tensor([1.0, 0.0]), t_u, t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what basically happens in Machine learning or Deep Learning in general. But points to note is that if we use larger learning rate lets say 1e-2 then the error will not converge. Not only that if we print out the gradients in each iterations we can see the values are large. These are potential problems we need to fix or mitigate.\n",
    "\n",
    "To do that, we can change learning rate or normalize input data (typical ML techniques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 completed with error value of 80.36434173583984 and gradient tensor([-77.6140, -10.6400])\n",
      "Iteration 501 completed with error value of 5.08172607421875 and gradient tensor([-0.2655,  1.5892])\n",
      "Iteration 1001 completed with error value of 2.984950065612793 and gradient tensor([-0.0461,  0.2944])\n",
      "Iteration 1501 completed with error value of 2.9287469387054443 and gradient tensor([-0.0064,  0.0416])\n",
      "Iteration 2001 completed with error value of 2.927666664123535 and gradient tensor([-0.0008,  0.0056])\n",
      "Iteration 2501 completed with error value of 2.927645683288574 and gradient tensor([-0.0001,  0.0008])\n",
      "Iteration 3001 completed with error value of 2.927644968032837 and gradient tensor([-2.3007e-05,  9.2626e-05])\n",
      "Iteration 3501 completed with error value of 2.927644968032837 and gradient tensor([-2.3007e-05,  9.2626e-05])\n",
      "Iteration 4001 completed with error value of 2.927644968032837 and gradient tensor([-2.3007e-05,  9.2626e-05])\n",
      "Iteration 4501 completed with error value of 2.927644968032837 and gradient tensor([-2.3007e-05,  9.2626e-05])\n"
     ]
    }
   ],
   "source": [
    "t_un = 0.1*t_u\n",
    "trained_params = training_loop(5000, 1e-2, torch.tensor([1.0, 0.0]), t_un, t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that values converge significantly and output values greatly match the input values to output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24955f51490>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#predictions on trained params\n",
    "t_p = model(t_un, trained_params[0], trained_params[1])\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy())\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the trained parameters fit the input data very nicely. \n",
    "\n",
    "One of the major problems of implementations like above is we need to manually keep track of the operations and calculate the differentiation operations to calculate the error propagation throughout the functions. Our example is simple, so we could do the calculations ourselves. But in large networks this is not practical.\n",
    "\n",
    "To solve this issue pytorch tensors provide us with autagrad functionality. (Threory behind this is not complex, but you need to find correct materials unless you are super smart to understand everything by reading wikipedia notes. XD). Because of this no matter how nested the forward calculation function is we can easily find derivatives to a related parameters. (In tensorflow we use `GradientTape` context manager for this by the way.)\n",
    "\n",
    "Usage of autograd to our above problem is as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_vals, w, b):\n",
    "    return w*input_vals + b\n",
    "\n",
    "def calc_error(pred_vals, true_vals):\n",
    "    return ((pred_vals-true_vals)**2).mean() # Since we are using tensors, or we can use numpy functions here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `requires_grad` parameter. This tells pytorch to keep track of all the graph of tensor operations built on top of the params tensors. So any tensor that originated from params can access the chain of functions which used to create itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have to do is do all the required operations and then call the backward function to calculate the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_vals = model(t_u, *params)\n",
    "error = calc_error(model(t_u*0.1, *params), t_c)\n",
    "error.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-77.6140, -10.6400])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, params now contain the error derivative with respective to the parameters.\n",
    "\n",
    "So demonstrated earlier we can calculate the derivative of any tensoe using `requires_grad` parameter.\n",
    "\n",
    "**But there's a catch!** backward function calculate and accumulate the gradients in each call. Which means if we call backward function multiple times leaf tensors will have inaccurate values. So we need to explicitly make the gradient to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if params.grad is not None:\n",
    "    params.grad.zero_()\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage of this technique for previous example is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_iter, learning_rate, params, input_vals, true_vals):\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "\n",
    "        w, b = params\n",
    "        preds = model(input_vals, w, b)\n",
    "        error = calc_error(preds, true_vals)\n",
    "\n",
    "        # Backpropagation or differenciation process\n",
    "        error.backward()\n",
    "\n",
    "        # Updating parameters to compensate for error gradients\n",
    "        with torch.no_grad():  # Have to use this condition to avoid graph building with param\n",
    "            params -= learning_rate * params.grad\n",
    "\n",
    "        if(i%500==0):\n",
    "            print(f\"Iteration {i+1} completed with error value of {error} and gradient {params.grad}\")\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 completed with error value of 80.36434173583984 and gradient tensor([-77.6140, -10.6400])\n",
      "Iteration 501 completed with error value of 7.843369007110596 and gradient tensor([-0.2248,  1.2726])\n",
      "Iteration 1001 completed with error value of 3.8254828453063965 and gradient tensor([-0.0961,  0.5439])\n",
      "Iteration 1501 completed with error value of 3.091630458831787 and gradient tensor([-0.0411,  0.2324])\n",
      "Iteration 2001 completed with error value of 2.9575960636138916 and gradient tensor([-0.0176,  0.0993])\n",
      "Iteration 2501 completed with error value of 2.9331159591674805 and gradient tensor([-0.0075,  0.0425])\n",
      "Iteration 3001 completed with error value of 2.9286458492279053 and gradient tensor([-0.0032,  0.0181])\n",
      "Iteration 3501 completed with error value of 2.9278290271759033 and gradient tensor([-0.0014,  0.0078])\n",
      "Iteration 4001 completed with error value of 2.9276793003082275 and gradient tensor([-0.0006,  0.0033])\n",
      "Iteration 4501 completed with error value of 2.927651882171631 and gradient tensor([-0.0002,  0.0014])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(5000, 1e-2, torch.tensor([1.0, 0.0], requires_grad=True), t_un, t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very convenient as we only require to build the forward pass.\n",
    "\n",
    "\n",
    "Also additionally pytorch provides a submodule which provide different optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional',\n",
       " '_multi_tensor',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can use this type optimizer to our models to improve efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "preds = model(t_u, *params)\n",
    "error = calc_error(preds, t_c)\n",
    "\n",
    "optimizer.zero_grad() # We need to manually reset optimizers as well!\n",
    "error.backward()\n",
    "optimizer.step()\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, optmizers update the parameters without we having to manually adjust the parameter values (substract change).\n",
    "\n",
    "\n",
    "> In scenarios we need to make sure autograd is not in effect we can use `with torch.no_grad()` context manager or `with torch.set_grad_enabled(val)` context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Pytorch_-_Use_Case_Exploration-FSDBtUdB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1153c933a691d5b7872e50b0da6113c7d23c17547b77ca34e6f51a7318bb0ae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
